# Local Voice Assistant

A privacy-focused, fully local voice assistant that listens to your voice, transcribes it, processes it with a local Large Language Model (LLM), and speaks back the response.

## Features

- **ğŸ™ï¸ Real-time Audio Recording**: Captures voice input via microphone.
- **ğŸ“ Local Speech-to-Text**: Uses [OpenAI Whisper](https://github.com/openai/whisper) for accurate transcription running locally.
- **ğŸ§  Local Intelligence**: Powered by [Ollama](https://ollama.com/) (running Llama 3.2) for private, offline responses.
- **ğŸ’¾ Long-term Memory**: Stores user details and conversations using Vector Search (FAISS).
- **ğŸ˜Š Emotion Detection**: Analyzes user sentiment to adjust responses.
- **ğŸ—£ï¸ Text-to-Speech**: Uses `pyttsx3` for offline voice synthesis.
- **ğŸ”’ Privacy First**: All processing happens on your machine. No data is sent to the cloud.

## Prerequisites

Before running the project, ensure you have the following installed:

1.  **Python 3.8+**
2.  **FFmpeg**: Required for Whisper audio processing.
    *   *Windows*: `winget install ffmpeg` or download from [gyan.dev](https://www.gyan.dev/ffmpeg/builds/).
    *   *Mac*: `brew install ffmpeg`
    *   *Linux*: `sudo apt install ffmpeg`
3.  **Ollama**: Download and install from [ollama.com](https://ollama.com/).
    *   Pull the model used in the code:
        ```bash
        ollama pull llama3.2:3b
        ```
    *   Ensure Ollama is running (`ollama serve` or via the desktop app).

## Installation

1.  **Clone the repository**:
    ```bash
    git clone <repository-url>
    cd AI-VOICE-AGENT
    ```

2.  **Create a virtual environment** (recommended):
    ```bash
    python -m venv venv
    # Windows
    venv\Scripts\activate
    # Mac/Linux
    source venv/bin/activate
    ```

3.  **Install dependencies**:
    ```bash
    pip install -r requirements.txt
    ```

## Usage

1.  **Start the Assistant**:
    ```bash
    python voice_agent/main.py
    ```

2.  **Interact**:
    *   Wait for the "Speak after the beep..." prompt.
    *   Speak your query clearly.
    *   The assistant will:
        *   Transcribe your speech.
        *   Detect your emotion and category (Personal, Work, etc.).
        *   Retrieve relevant past memories.
        *   Generate a context-aware response.
        *   Speak it back to you.

## Project Structure

```
AI-VOICE-AGENT/
â”œâ”€â”€ voice_agent/
â”‚   â”œâ”€â”€ asr/            # Automatic Speech Recognition (Whisper)
â”‚   â”œâ”€â”€ audio/          # Audio recording logic
â”‚   â”œâ”€â”€ llm/            # Local LLM integration (Ollama)
â”‚   â”œâ”€â”€ memory/         # Memory & Emotion Detection
â”‚   â”‚   â”œâ”€â”€ category_detector.py
â”‚   â”‚   â”œâ”€â”€ emotion_detector.py
â”‚   â”‚   â”œâ”€â”€ memory.py
â”‚   â”‚   â””â”€â”€ vector_memory.py
â”‚   â”œâ”€â”€ tts/            # Text-to-Speech engine
â”‚   â””â”€â”€ main.py         # Entry point
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md           # Project documentation
```

## Configuration

- **Model Selection**: To change the LLM model, edit `voice_agent/llm/local_llm.py` and update the `"model"` field in the payload (e.g., to `llama3`, `mistral`, etc.).
- **Recording Duration**: Adjust the `duration` parameter in `voice_agent/main.py` to change how long the assistant listens.

## License

[MIT License](LICENSE)
